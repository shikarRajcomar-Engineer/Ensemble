{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Advanced Augmentation.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOW5ZivCASFqA6jAKWwWGvQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shikarRajcomar-Engineer/Ensemble/blob/master/Advanced_Augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzGFekrG8tgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iPBPO9MN6qH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "6a2f7b7e-13bb-49d2-978f-dbdff3e64323"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teM_sn5eZ3vs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "path='/content/drive/My Drive/B1-Msc experiment/'    \n",
        "Results=pd.DataFrame([])\n",
        "Results1=pd.DataFrame([])\n",
        "Results2=pd.DataFrame([])\n",
        "Results3=pd.DataFrame([])\n",
        "Results4=pd.DataFrame([])\n",
        "Results5=pd.DataFrame([])\n",
        "Results6=pd.DataFrame([])\n",
        "\n",
        "aResults=pd.DataFrame([])\n",
        "aResults1=pd.DataFrame([])\n",
        "aResults2=pd.DataFrame([])\n",
        "aResults3=pd.DataFrame([])\n",
        "aResults4=pd.DataFrame([])\n",
        "aResults5=pd.DataFrame([])\n",
        "aResults6=pd.DataFrame([])\n",
        "\n",
        "bResults=pd.DataFrame([])\n",
        "bResults1=pd.DataFrame([])\n",
        "bResults2=pd.DataFrame([])\n",
        "bResults3=pd.DataFrame([])\n",
        "bResults4=pd.DataFrame([])\n",
        "bResults5=pd.DataFrame([])\n",
        "bResults6=pd.DataFrame([])\n",
        "\n",
        "cResults=pd.DataFrame([])\n",
        "cResults1=pd.DataFrame([])\n",
        "cResults2=pd.DataFrame([])\n",
        "cResults3=pd.DataFrame([])\n",
        "cResults4=pd.DataFrame([])\n",
        "cResults5=pd.DataFrame([])\n",
        "cResults6=pd.DataFrame([])\n",
        "\n",
        "dResults=pd.DataFrame([])\n",
        "dResults1=pd.DataFrame([])\n",
        "dResults2=pd.DataFrame([])\n",
        "dResults3=pd.DataFrame([])\n",
        "dResults4=pd.DataFrame([])\n",
        "dResults5=pd.DataFrame([])\n",
        "dResults6=pd.DataFrame([])\n",
        "\n",
        "eResults=pd.DataFrame([])\n",
        "eResults1=pd.DataFrame([])\n",
        "eResults2=pd.DataFrame([])\n",
        "eResults3=pd.DataFrame([])\n",
        "eResults4=pd.DataFrame([])\n",
        "eResults5=pd.DataFrame([])\n",
        "eResults6=pd.DataFrame([])\n",
        "\n",
        "fResults=pd.DataFrame([])\n",
        "fResults1=pd.DataFrame([])\n",
        "fResults2=pd.DataFrame([])\n",
        "fResults3=pd.DataFrame([])\n",
        "fResults4=pd.DataFrame([])\n",
        "fResults5=pd.DataFrame([])\n",
        "fResults6=pd.DataFrame([])\n",
        "\n",
        "\n",
        "\n",
        "Dataset7=['candy224']\n",
        "Dataset8=['composition_vii224']\n",
        "Dataset9=['feathers224']\n",
        "Dataset10=['la_muse224']\n",
        "Dataset11=['mosaic224']\n",
        "Dataset12=['starry_night224']\n",
        "Dataset13=['the_scream224']\n",
        "# Dataset14=['the_wave224']\n",
        "# Dataset15=['udnie224']\n",
        "# Dataset16=['samplepair224']\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq6LURpuydwY",
        "colab_type": "text"
      },
      "source": [
        "# candy224"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TKkybAB-scy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "outputId": "2e73bdc4-9d75-4104-ee58-4bf48c5bf4c0"
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "path='/content/drive/My Drive/B1-Msc experiment/'    \n",
        "\n",
        "\n",
        "\n",
        "batch_size=32\n",
        "steps=100\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['No Aug','No Aug','No Aug','No Aug']\n",
        "df['dataset']=['candy224','candy224','candy224','candy224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "Results=Results.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(Results)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train: 0.759, Test: 0.776\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Class 0</th>\n",
              "      <th>Class 1</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>macro avg</th>\n",
              "      <th>weighted avg</th>\n",
              "      <th>aug</th>\n",
              "      <th>dataset</th>\n",
              "      <th>val acc</th>\n",
              "      <th>acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.941176</td>\n",
              "      <td>0.707317</td>\n",
              "      <td>0.775862</td>\n",
              "      <td>0.824247</td>\n",
              "      <td>0.820215</td>\n",
              "      <td>No Aug</td>\n",
              "      <td>Original</td>\n",
              "      <td>67.241377</td>\n",
              "      <td>72.932333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.966667</td>\n",
              "      <td>0.775862</td>\n",
              "      <td>0.769048</td>\n",
              "      <td>0.775862</td>\n",
              "      <td>No Aug</td>\n",
              "      <td>Original</td>\n",
              "      <td>67.241377</td>\n",
              "      <td>72.932333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1-score</th>\n",
              "      <td>0.711111</td>\n",
              "      <td>0.816901</td>\n",
              "      <td>0.775862</td>\n",
              "      <td>0.764006</td>\n",
              "      <td>0.765830</td>\n",
              "      <td>No Aug</td>\n",
              "      <td>Original</td>\n",
              "      <td>67.241377</td>\n",
              "      <td>72.932333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>support</th>\n",
              "      <td>28.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>0.775862</td>\n",
              "      <td>58.000000</td>\n",
              "      <td>58.000000</td>\n",
              "      <td>No Aug</td>\n",
              "      <td>Original</td>\n",
              "      <td>67.241377</td>\n",
              "      <td>72.932333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Class 0    Class 1  accuracy  macro avg  weighted avg     aug  \\\n",
              "precision   0.941176   0.707317  0.775862   0.824247      0.820215  No Aug   \n",
              "recall      0.571429   0.966667  0.775862   0.769048      0.775862  No Aug   \n",
              "f1-score    0.711111   0.816901  0.775862   0.764006      0.765830  No Aug   \n",
              "support    28.000000  30.000000  0.775862  58.000000     58.000000  No Aug   \n",
              "\n",
              "            dataset    val acc        acc  \n",
              "precision  Original  67.241377  72.932333  \n",
              "recall     Original  67.241377  72.932333  \n",
              "f1-score   Original  67.241377  72.932333  \n",
              "support    Original  67.241377  72.932333  "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kJgPaZdSQcow",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(horizontal_flip=True)\n",
        "val_datagen =  ImageDataGenerator(horizontal_flip=True)\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['HorizontalFlip','HorizontalFlip','HorizontalFlip','HorizontalFlip']\n",
        "df['dataset']=['candy224','candy224','candy224','candy224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "Results1=Results1.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(Results1)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m7OuDLQcSSjF",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(vertical_flip=True)\n",
        "val_datagen =  ImageDataGenerator(vertical_flip=True)\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['vertialFLip','vertialFLip','vertialFLip','vertialFLip']\n",
        "df['dataset']=['candy224','candy224','candy224','candy224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "Results2=Results2.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(Results2)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4EHx6wZZU3VM",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(rotation_range=(random.randrange(0,360,30)))\n",
        "val_datagen =  ImageDataGenerator(rotation_range=(random.randrange(0,360,30)))\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['Rotate','Rotate','Rotate','Rotate']\n",
        "df['dataset']=['candy224','candy224','candy224','candy224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "Results3=Results3.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(Results3)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YvSdFko2VD6F",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(shear_range=(random.randrange(0,100,10)))\n",
        "val_datagen =  ImageDataGenerator(shear_range=(random.randrange(0,100,10)))\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['Shear','Shear','Shear','Shear']\n",
        "df['dataset']=['candy224','candy224','candy224','candy224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "Results4=Results4.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(Results4)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dDhytiNfVSIU",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(width_shift_range=(random.uniform(0.1,0.9)))\n",
        "val_datagen =  ImageDataGenerator(width_shift_range=(random.uniform(0.1,0.9)))\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['Width','Width','Width','Width']\n",
        "df['dataset']=['candy224','candy224','candy224','candy224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "Results5=Results5.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(Results5)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IHTIcbDQVeN2",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(height_shift_range=(random.uniform(0.1,0.9)))\n",
        "val_datagen =  ImageDataGenerator(height_shift_range=(random.uniform(0.1,0.9)))\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['height','height','height','height']\n",
        "df['dataset']=['candy224','candy224','candy224','candy224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "Results6=Results6.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(Results6)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haxUweSwumb2",
        "colab_type": "text"
      },
      "source": [
        "# composition_vii224 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cZur2kdBuy4P",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "batch_size=32\n",
        "steps=100\n",
        "\n",
        "\n",
        "hist_results=[]\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset1[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['No Aug','No Aug','No Aug','No Aug']\n",
        "df['dataset']=['composition_vii224','composition_vii224','composition_vii224','composition_vii224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "aResults=aResults.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(aResults)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4YDLeaU0uy4V",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset1[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(horizontal_flip=True)\n",
        "val_datagen =  ImageDataGenerator(horizontal_flip=True)\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['HorizontalFlip','HorizontalFlip','HorizontalFlip','HorizontalFlip']\n",
        "df['dataset']=['composition_vii224','composition_vii224','composition_vii224','composition_vii224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "aResults1=aResults1.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(aResults1)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g1Xbn7BEuy4b",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset1[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(vertical_flip=True)\n",
        "val_datagen =  ImageDataGenerator(vertical_flip=True)\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['vertialFLip','vertialFLip','vertialFLip','vertialFLip']\n",
        "df['dataset']=['composition_vii224','composition_vii224','composition_vii224','composition_vii224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "aResults2=aResults2.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(aResults2)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xymNX4T6uy4g",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset1[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(rotation_range=(random.randrange(0,360,30)))\n",
        "val_datagen =  ImageDataGenerator(rotation_range=(random.randrange(0,360,30)))\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['Rotate','Rotate','Rotate','Rotate']\n",
        "df['dataset']=['composition_vii224','composition_vii224','composition_vii224','composition_vii224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "aResults3=aResults3.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(aResults3)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fwH4LUcguy4n",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset1[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(shear_range=(random.randrange(0,100,10)))\n",
        "val_datagen =  ImageDataGenerator(shear_range=(random.randrange(0,100,10)))\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['Shear','Shear','Shear','Shear']\n",
        "df['dataset']=['composition_vii224','composition_vii224','composition_vii224','composition_vii224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "aResults4=aResults4.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(aResults4)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LMXl97N6uy4t",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset1[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(width_shift_range=(random.uniform(0.1,0.9)))\n",
        "val_datagen =  ImageDataGenerator(width_shift_range=(random.uniform(0.1,0.9)))\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['Width','Width','Width','Width']\n",
        "df['dataset']=['composition_vii224','composition_vii224','composition_vii224','composition_vii224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "aResults5=aResults5.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(aResults5)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FQT-8tS5uy4x",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset1[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(height_shift_range=(random.uniform(0.1,0.9)))\n",
        "val_datagen =  ImageDataGenerator(height_shift_range=(random.uniform(0.1,0.9)))\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['height','height','height','height']\n",
        "df['dataset']=['composition_vii224','composition_vii224','composition_vii224','composition_vii224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "aResults6=aResults6.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(aResults6)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki_cDK5LxVk4",
        "colab_type": "text"
      },
      "source": [
        "# Vertical Flipping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8TFKNPFHxemm",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "batch_size=32\n",
        "steps=100\n",
        "\n",
        "\n",
        "hist_results=[]\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset2[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['No Aug','No Aug','No Aug','No Aug']\n",
        "df['dataset']=['Vflip224','Vflip224','Vflip224','Vflip224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "bResults=bResults.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(bResults)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kA8U25Pvxems",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset2[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(horizontal_flip=True)\n",
        "val_datagen =  ImageDataGenerator(horizontal_flip=True)\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['HorizontalFlip','HorizontalFlip','HorizontalFlip','HorizontalFlip']\n",
        "df['dataset']=['Vflip224','Vflip224','Vflip224','Vflip224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "bResults1=bResults1.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(bResults1)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nQ0wkBnHxemw",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset2[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(vertical_flip=True)\n",
        "val_datagen =  ImageDataGenerator(vertical_flip=True)\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['vertialFLip','vertialFLip','vertialFLip','vertialFLip']\n",
        "df['dataset']=['Vflip224','Vflip224','Vflip224','Vflip224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "bResults2=bResults2.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(bResults2)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6MU-zxKrxem0",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset2[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(rotation_range=(random.randrange(0,360,30)))\n",
        "val_datagen =  ImageDataGenerator(rotation_range=(random.randrange(0,360,30)))\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['Rotate','Rotate','Rotate','Rotate']\n",
        "df['dataset']=['Vflip224','Vflip224','Vflip224','Vflip224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "bResults3=bResults3.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(bResults3)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DkqC7WtDxem4",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset2[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(shear_range=(random.randrange(0,100,10)))\n",
        "val_datagen =  ImageDataGenerator(shear_range=(random.randrange(0,100,10)))\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['Shear','Shear','Shear','Shear']\n",
        "df['dataset']=['Vflip224','Vflip224','Vflip224','Vflip224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "bResults4=bResults4.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(bResults4)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TkLDLeunxem-",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset2[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(width_shift_range=(random.uniform(0.1,0.9)))\n",
        "val_datagen =  ImageDataGenerator(width_shift_range=(random.uniform(0.1,0.9)))\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['Width','Width','Width','Width']\n",
        "df['dataset']=['Vflip224','Vflip224','Vflip224','Vflip224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "bResults5=bResults5.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(bResults5)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qUGW5RYExenE",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset2[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(height_shift_range=(random.uniform(0.1,0.9)))\n",
        "val_datagen =  ImageDataGenerator(height_shift_range=(random.uniform(0.1,0.9)))\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['height','height','height','height']\n",
        "df['dataset']=['Vflip224','Vflip224','Vflip224','Vflip224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "bResults6=bResults6.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(Results6)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL6xR88AyOKJ",
        "colab_type": "text"
      },
      "source": [
        "# la_muse224 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_a3Ba1rT1QT-",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch_size=32\n",
        "steps=100\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset3[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['No Aug','No Aug','No Aug','No Aug']\n",
        "df['dataset']=['la_muse224','la_muse224','la_muse224','la_muse224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "cResults=cResults.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(cResults)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7eugvyBn1QUM",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset3[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(horizontal_flip=True)\n",
        "val_datagen =  ImageDataGenerator(horizontal_flip=True)\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['HorizontalFlip','HorizontalFlip','HorizontalFlip','HorizontalFlip']\n",
        "df['dataset']=['la_muse224','la_muse224','la_muse224','la_muse224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "cResults1=cResults1.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(cResults1)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6o9EztMM1QUX",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset3[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(vertical_flip=True)\n",
        "val_datagen =  ImageDataGenerator(vertical_flip=True)\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['vertialFLip','vertialFLip','vertialFLip','vertialFLip']\n",
        "df['dataset']=['la_muse224','la_muse224','la_muse224','la_muse224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "cResults2=cResults2.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(cResults2)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ujVo04Nc1QUi",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset3[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(rotation_range=(random.randrange(0,360,30)))\n",
        "val_datagen =  ImageDataGenerator(rotation_range=(random.randrange(0,360,30)))\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['Rotate','Rotate','Rotate','Rotate']\n",
        "df['dataset']=['la_muse224','la_muse224','la_muse224','la_muse224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "cResults3=cResults3.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(cResults3)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tdbcoLNx1QUs",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset3[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(shear_range=(random.randrange(0,100,10)))\n",
        "val_datagen =  ImageDataGenerator(shear_range=(random.randrange(0,100,10)))\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['Shear','Shear','Shear','Shear']\n",
        "df['dataset']=['la_muse224','la_muse224','la_muse224','la_muse224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "cResults4=cResults4.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(cResults4)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kdygRTdR1QU2",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset3[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(width_shift_range=(random.uniform(0.1,0.9)))\n",
        "val_datagen =  ImageDataGenerator(width_shift_range=(random.uniform(0.1,0.9)))\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['Width','Width','Width','Width']\n",
        "df['dataset']=['la_muse224','la_muse224','la_muse224','la_muse224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "cResults5=cResults5.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(cResults5)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6RMqYimf1QVA",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset3[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(height_shift_range=(random.uniform(0.1,0.9)))\n",
        "val_datagen =  ImageDataGenerator(height_shift_range=(random.uniform(0.1,0.9)))\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['height','height','height','height']\n",
        "df['dataset']=['la_muse224','la_muse224','la_muse224','la_muse224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "cResults6=cResults6.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(cResults6)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKmt13D01bjd",
        "colab_type": "text"
      },
      "source": [
        "#mosaic224 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A8F70gRt2dzh",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch_size=32\n",
        "steps=100\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset4[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['No Aug','No Aug','No Aug','No Aug']\n",
        "df['dataset']=['mosaic224','mosaic224','mosaic224','mosaic224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "dResults=dResults.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(dResults)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UojuEJWA2dzz",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset4[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(horizontal_flip=True)\n",
        "val_datagen =  ImageDataGenerator(horizontal_flip=True)\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['HorizontalFlip','HorizontalFlip','HorizontalFlip','HorizontalFlip']\n",
        "df['dataset']=['mosaic224','mosaic224','mosaic224','mosaic224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "dResults1=dResults1.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(dResults1)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1GvHUNR32d0D",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset4[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(vertical_flip=True)\n",
        "val_datagen =  ImageDataGenerator(vertical_flip=True)\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['vertialFLip','vertialFLip','vertialFLip','vertialFLip']\n",
        "df['dataset']=['mosaic224','mosaic224','mosaic224','mosaic224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "dResults2=dResults2.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(dResults2)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "86p_bEQV2d0Y",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset4[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(rotation_range=(random.randrange(0,360,30)))\n",
        "val_datagen =  ImageDataGenerator(rotation_range=(random.randrange(0,360,30)))\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['Rotate','Rotate','Rotate','Rotate']\n",
        "df['dataset']=['mosaic224','mosaic224','mosaic224','mosaic224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "dResults3=dResults3.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(dResults3)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ob0JJhUn2d0p",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset4[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(shear_range=(random.randrange(0,100,10)))\n",
        "val_datagen =  ImageDataGenerator(shear_range=(random.randrange(0,100,10)))\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['Shear','Shear','Shear','Shear']\n",
        "df['dataset']=['mosaic224','mosaic224','mosaic224','mosaic224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "dResults4=dResults4.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(dResults4)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9JffBjJA2d06",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset4[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(width_shift_range=(random.uniform(0.1,0.9)))\n",
        "val_datagen =  ImageDataGenerator(width_shift_range=(random.uniform(0.1,0.9)))\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['Width','Width','Width','Width']\n",
        "df['dataset']=['mosaic224','mosaic224','mosaic224','mosaic224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "dResults5=dResults5.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(dResults5)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I34DGu2Y2d1H",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset4[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(height_shift_range=(random.uniform(0.1,0.9)))\n",
        "val_datagen =  ImageDataGenerator(height_shift_range=(random.uniform(0.1,0.9)))\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['height','height','height','height']\n",
        "df['dataset']=['mosaic224','mosaic224','mosaic224','mosaic224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "dResults6=dResults6.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(dResults6)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gXmS0xZ21wD",
        "colab_type": "text"
      },
      "source": [
        "# starry_night224 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDK9msi7252z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7lIibO6J4RvW",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch_size=32\n",
        "steps=100\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset5[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['No Aug','No Aug','No Aug','No Aug']\n",
        "df['dataset']=['starry_night224','starry_night224','starry_night224','starry_night224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "eResults=eResults.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(eResults)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OiP2B9dZ4Rv-",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset5[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(horizontal_flip=True)\n",
        "val_datagen =  ImageDataGenerator(horizontal_flip=True)\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['HorizontalFlip','HorizontalFlip','HorizontalFlip','HorizontalFlip']\n",
        "df['dataset']=['starry_night224','starry_night224','starry_night224','starry_night224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "eResults1=eResults1.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(eResults1)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RV2c6BBS4RwU",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset5[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(vertical_flip=True)\n",
        "val_datagen =  ImageDataGenerator(vertical_flip=True)\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['vertialFLip','vertialFLip','vertialFLip','vertialFLip']\n",
        "df['dataset']=['starry_night224','starry_night224','starry_night224','starry_night224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "eResults2=eResults2.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(eResults2)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qwLtRFIp4Rwm",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset5[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(rotation_range=(random.randrange(0,360,30)))\n",
        "val_datagen =  ImageDataGenerator(rotation_range=(random.randrange(0,360,30)))\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['Rotate','Rotate','Rotate','Rotate']\n",
        "df['dataset']=['starry_night224','starry_night224','starry_night224','starry_night224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "eResults3=eResults3.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(eResults3)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cE18CAsD4Rw8",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset5[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(shear_range=(random.randrange(0,100,10)))\n",
        "val_datagen =  ImageDataGenerator(shear_range=(random.randrange(0,100,10)))\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['Shear','Shear','Shear','Shear']\n",
        "df['dataset']=['starry_night224','starry_night224','starry_night224','starry_night224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "eResults4=eResults4.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(eResults4)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XHhZerck4RxJ",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset5[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(width_shift_range=(random.uniform(0.1,0.9)))\n",
        "val_datagen =  ImageDataGenerator(width_shift_range=(random.uniform(0.1,0.9)))\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['Width','Width','Width','Width']\n",
        "df['dataset']=['starry_night224','starry_night224','starry_night224','starry_night224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "eResults5=eResults5.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(eResults5)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LLqCpZyi4Rxa",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset5[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(height_shift_range=(random.uniform(0.1,0.9)))\n",
        "val_datagen =  ImageDataGenerator(height_shift_range=(random.uniform(0.1,0.9)))\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['height','height','height','height']\n",
        "df['dataset']=['starry_night224','starry_night224','starry_night224','starry_night224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "eResults6=eResults6.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(eResults6)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2JhSX9j4Vtj",
        "colab_type": "text"
      },
      "source": [
        "#the_scream224 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "grAtWEfz5tY-",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch_size=32\n",
        "steps=100\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset5[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['No Aug','No Aug','No Aug','No Aug']\n",
        "df['dataset']=['the_scream224','the_scream224','the_scream224','the_scream224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "eResults=eResults.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(eResults)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W12gI49C5tZf",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset5[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(horizontal_flip=True)\n",
        "val_datagen =  ImageDataGenerator(horizontal_flip=True)\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['HorizontalFlip','HorizontalFlip','HorizontalFlip','HorizontalFlip']\n",
        "df['dataset']=['the_scream224','the_scream224','the_scream224','the_scream224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "eResults1=eResults1.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(eResults1)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y2Mz-rjD5tZt",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset5[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(vertical_flip=True)\n",
        "val_datagen =  ImageDataGenerator(vertical_flip=True)\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['vertialFLip','vertialFLip','vertialFLip','vertialFLip']\n",
        "df['dataset']=['the_scream224','the_scream224','the_scream224','the_scream224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "eResults2=eResults2.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(eResults2)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xxmMjMgl5tZ-",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset5[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(rotation_range=(random.randrange(0,360,30)))\n",
        "val_datagen =  ImageDataGenerator(rotation_range=(random.randrange(0,360,30)))\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['Rotate','Rotate','Rotate','Rotate']\n",
        "df['dataset']=['the_scream224','the_scream224','the_scream224','the_scream224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "eResults3=eResults3.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(eResults3)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cxY6q36I5taQ",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset5[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(shear_range=(random.randrange(0,100,10)))\n",
        "val_datagen =  ImageDataGenerator(shear_range=(random.randrange(0,100,10)))\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['Shear','Shear','Shear','Shear']\n",
        "df['dataset']=['the_scream224','the_scream224','the_scream224','the_scream224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "eResults4=eResults4.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(eResults4)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ea1ZJi4D5tai",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset5[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(width_shift_range=(random.uniform(0.1,0.9)))\n",
        "val_datagen =  ImageDataGenerator(width_shift_range=(random.uniform(0.1,0.9)))\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['Width','Width','Width','Width']\n",
        "df['dataset']=['the_scream224','the_scream224','the_scream224','the_scream224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "eResults5=eResults5.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(eResults5)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q_33vk9x5taz",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(2)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "import numpy as np \n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import sys\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras import backend as K\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from numpy.random import seed\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "import sklearn\n",
        "import datetime, os\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from plotly import tools\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import datetime as datetime\n",
        "import tensorflow\n",
        "from skimage import transform as tf\n",
        "import random\n",
        "from keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=3)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=3\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "def load_data_training_and_test(datasetname):\n",
        "        npzfile=np.load(path+datasetname+'training_data.npz')\n",
        "        train=npzfile['arr_0']\n",
        "        \n",
        "\n",
        "        npzfile=np.load(path+datasetname+'training_labels.npz')\n",
        "        train_labels=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_data.npz')\n",
        "        test=npzfile['arr_0']\n",
        "\n",
        "        npzfile=np.load(path+datasetname+'testing_labels.npz')\n",
        "        test_labels=npzfile['arr_0']    \n",
        "        return (train,train_labels),(test,test_labels)\n",
        "\n",
        "(x_train,y_train),(x_test,y_testing)=load_data_training_and_test(Dataset5[0])\n",
        "\n",
        "\n",
        "y_train=y_train.reshape(y_train.shape[0],1)\n",
        "y_testing=y_testing.reshape(y_testing.shape[0],1)\n",
        "\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "training_size=len(x_train)\n",
        "test_size=len(y_testing)\n",
        "classes = np.unique(y_train)\n",
        "nClasses = len(classes)\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "# model = VGG16()\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(160, (3, 3), kernel_initializer=keras.initializers.glorot_uniform(seed=2), kernel_regularizer=l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"softmax\"))\n",
        "  # from keras.optimizers import SGD\n",
        "  # opt = SGD(lr=0.001)\n",
        "  # model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt,\n",
        "  #             metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "datagen = ImageDataGenerator(height_shift_range=(random.uniform(0.1,0.9)))\n",
        "val_datagen =  ImageDataGenerator(height_shift_range=(random.uniform(0.1,0.9)))\n",
        "\n",
        "# Define Callback function for early stopping and tensorboard\n",
        "earlystop=EarlyStopping(patience=5)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy'\n",
        "                                        ,patience=5\n",
        "                                        ,verbose=0\n",
        "                                        ,factor=0.5\n",
        "                                        ,min_lr=0.00001)\n",
        "\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "\n",
        "\n",
        "epochs=50\n",
        "\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "\n",
        "# fit model\n",
        "train_generator = datagen.flow(x_train,y_train,batch_size=batch_size)\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "validation_generator = val_datagen.flow(x_test,y_testing,batch_size=batch_size)\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      verbose=0,\n",
        "      shuffle=True\n",
        "      ,callbacks=callbacks)\n",
        "     \n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(history.epoch) + 1)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Train Set')\n",
        "plt.plot(epochs_range, val_acc, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Train Set')\n",
        "plt.plot(epochs_range, val_loss, label='Val Set')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_testing, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "his_df=pd.DataFrame(history.history)\n",
        "\n",
        "predicted_classes = model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "\n",
        "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
        "report_dict=classification_report(y_testing, predicted_classes, target_names=target_names,output_dict=True)\n",
        "df=pd.DataFrame(report_dict)\n",
        "df['aug']=['height','height','height','height']\n",
        "df['dataset']=['the_scream224','the_scream224','the_scream224','the_scream224']\n",
        "df['val acc']=(history.history['val_accuracy'][-1])*100\n",
        "df['acc']=(history.history['accuracy'][-1])*100\n",
        "eResults6=eResults6.append(df)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(eResults6)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlnvJY4K6QJq",
        "colab_type": "text"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDHHrwlf6V_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Final=pd.DataFrame([])\n",
        "Final=Final.append(Results)\n",
        "Final=Final.append(Results1)\n",
        "Final=Final.append(Results2)\n",
        "Final=Final.append(Results3)\n",
        "Final=Final.append(Results4)\n",
        "Final=Final.append(Results5)\n",
        "Final=Final.append(Results6)\n",
        "\n",
        "Final=Final.append(aResults)\n",
        "Final=Final.append(aResults1)\n",
        "Final=Final.append(aResults2)\n",
        "Final=Final.append(aResults3)\n",
        "Final=Final.append(aResults4)\n",
        "Final=Final.append(aResults5)\n",
        "Final=Final.append(aResults6)\n",
        "\n",
        "Final=Final.append(bResults)\n",
        "Final=Final.append(bResults1)\n",
        "Final=Final.append(bResults2)\n",
        "Final=Final.append(bResults3)\n",
        "Final=Final.append(bResults4)\n",
        "Final=Final.append(bResults5)\n",
        "Final=Final.append(bResults6)\n",
        "\n",
        "Final=Final.append(cResults)\n",
        "Final=Final.append(cResults1)\n",
        "Final=Final.append(cResults2)\n",
        "Final=Final.append(cResults3)\n",
        "Final=Final.append(cResults4)\n",
        "Final=Final.append(cResults5)\n",
        "Final=Final.append(cResults6)\n",
        "\n",
        "Final=Final.append(dResults)\n",
        "Final=Final.append(dResults1)\n",
        "Final=Final.append(dResults2)\n",
        "Final=Final.append(dResults3)\n",
        "Final=Final.append(dResults4)\n",
        "Final=Final.append(dResults5)\n",
        "Final=Final.append(dResults6)\n",
        "\n",
        "Final=Final.append(eResults)\n",
        "Final=Final.append(eResults1)\n",
        "Final=Final.append(eResults2)\n",
        "Final=Final.append(eResults3)\n",
        "Final=Final.append(eResults4)\n",
        "Final=Final.append(eResults5)\n",
        "Final=Final.append(eResults6)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "display(Final)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}